{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "37puETfgRzzg"
   },
   "source": [
    "# Data Preprocessing Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EoRP98MpR-qj"
   },
   "source": [
    "## Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전처리 1단계\n",
    "dataset = pd.read_csv('Data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['No', 'Yes', 'No', 'No', 'Yes', 'Yes', 'No', 'Yes', 'No', 'Yes'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#전처리 2단계\n",
    "#두 개의 엔티티를 생성 \n",
    "    # 하나는 특징 행렬, 다른 하나는 종속 변수 벡터\n",
    "# 머신러닝 모델을 훈련시킬 어떤 데이터 set이든 같은 요소를 가지고 있음\n",
    "# 1.feature, 2. 종속 변수 벡터\n",
    "    # feature : 종속 변수를 예측할 때 사용될 cols (주로 첫 번째 col)\n",
    "    # 종속 변수 벡터 : 예측하고 싶은 data (주로 마지막 col)\n",
    "# 생성하고 싶은 두 개의 Entity는 \n",
    "    # 1. 'Country', 'Age', 'Salary' 이 세가지 cols를 별도로 포함하는 특징 행렬 (feature matrix)\n",
    "    # 2. 마지막 col만 포함하는 종속 변수 벡터 생성 ( 우리가 예측하고 싶은 부분 )\n",
    "    \n",
    "# 두 개의 entity 생성 \n",
    "#(entity : 실제, 독립채, 데이터 모델링에서 사용되는 객체, 유용한 정보를 저장하고 관리하기 위한 Thing)\n",
    "\n",
    "# feature matrix \n",
    "x = dataset.iloc[:] #locate index, 추출하고 싶은 cols의 index를 가져옴. cols뿐만 아니라 row index도 가져옴\n",
    "x\n",
    "\n",
    "# 마지막 열을 제거한 모든 col을 불러오기 위한 방법\n",
    "# 인데스로 어떤 col을 선택하고 싶은지 지정하고, col에서 가져온 row를 분리가능. 콤마로. [( , ) : col을 다룰 수 있음]\n",
    "x = dataset.iloc[ : , : -1].values\n",
    "\n",
    "# tip : python의 범위는 하한계인 0을 포함하지만, 상한계는 포함하지않아 index -1을 제외함.\n",
    "# 종속 변수 벡터 y\n",
    "y = dataset.iloc[ : , -1 ].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['France' 44.0 72000.0]\n",
      " ['Spain' 27.0 48000.0]\n",
      " ['Germany' 30.0 54000.0]\n",
      " ['Spain' 38.0 61000.0]\n",
      " ['Germany' 40.0 nan]\n",
      " ['France' 35.0 58000.0]\n",
      " ['Spain' nan 52000.0]\n",
      " ['France' 48.0 79000.0]\n",
      " ['Germany' 50.0 83000.0]\n",
      " ['France' 37.0 67000.0]]\n"
     ]
    }
   ],
   "source": [
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['No' 'Yes' 'No' 'No' 'Yes' 'Yes' 'No' 'Yes' 'No' 'Yes']\n"
     ]
    }
   ],
   "source": [
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 두 가지 Entity를 생성해야하는 이유\n",
    "    # 나중에 만들 machine model에 두 entity가 입력될 것이라 예상되기 때문\n",
    "    # model을 만들 때 몇몇 class를 사용할 건데, 이 class들은 dataset 전체를 원하지 않고\n",
    "    # 두 개의 분리된 entity를 원하기 때문"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nhfKXNxlSabC"
   },
   "source": [
    "## Taking care of missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#전처리 3단계\n",
    "# 누락된 데이터 처리\n",
    "# 사이킷런을 사용해서 누락된 데이터를 처리\n",
    "# simpleimputer 클래스를 불러와서 , 클래스 object인 인스턴스를 생성, 이 객체는 'Salary'의 평균값으로 결측치를 대체\n",
    "# 목표 ㅣ 누락된 데이터가 없는 새로운 feature matrix를 갖게 되고 'Salary'의 평균값이 결측값을 대체하게 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tip : class와 object\n",
    "# class : 제품의 설게도\n",
    "# object : 하나의 설계도로 만든 여러개의 제품\n",
    "# attribution : class안의 변수\n",
    "    # self.name = name (함수로부터 받은 name을 속성으로 할당할겠다.)\n",
    "# method : class 안의 함수\n",
    "# constructor(생성자) : 객체를 만들 때 가장먼저 실행되는 함수\n",
    "    # object = className(\"constructor 호출\")\n",
    "    # object가 self 매게변수 자리로 들어간다.\n",
    "# instance : memory에 실제로 살아서 실행되고 있는 object (instance는 object에 포함되어 있다.)\n",
    "    #사용법\n",
    "    # object = className()\n",
    "    # object.say()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "imputer = SimpleImputer(missing_values = np.nan, strategy = 'mean') \n",
    "# 첫번째로 대체해야 할 결측치를 지정해야 함.(첫 번째 인자)\n",
    "    # np.nan : dataset에 있는 모든 결측치를 대체하는 것. (empty value)\n",
    "# 두번째로 dataset의 결측치가 평균값으로 대체된다는 인자를 추가 \n",
    "\n",
    "#fit method가 하는일\n",
    "#fit method를 feature matrix에 연결 -> 'fit' method가 'Salary' col에 있는 결측치를 보고 '평균값을 계산'\n",
    "\n",
    "# transform method 호출\n",
    "# 누락된 'Salary'를 '평균 Salary'로 대체"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 우선 feature matrix X를 가져옴. 거기서 누락된 데이터를 대체하려고 하는 것이기 때문\n",
    "# 다음 X의 row을 먼저 봄\n",
    "# fit method는 이 method 내에 지정된 전체 col을 읽고 누락된 데이터를 찾아 모든 col을 지정할 수 있다.\n",
    "# 문자열을 찾는 경우 오류가 발생할 수 있어 숫자열만 지정 (실제로는 모든 숫자열을 포함해야함)\n",
    "imputer.fit(X[ : , 1:3]) # 오직 숫자만 인자로 받음, fit method가 'age', 'Salary' col의 모든 결측치를 찾아줄 거임.\n",
    "imputer.transform(x[ : , 1:3]) # 누락된 'Salary'를 'Salary'의 평균값으로 대체해 줄 것임\n",
    "\n",
    "# 주의할 점 : transform은 누락된 'Salary'와 'Age'가 대체된 새로 update된 버전의 X를 돌려준다는 것임\n",
    "    #그래서 지금 해야하는 작업은 feature matrix X를 업데이트하는 것임.\n",
    "        # transform이 두개의 col에 대체값을 채워 return하기 때문에\n",
    "            # X를 업데이트해서 feature matrix의 2번째와 3번째 col을 가져오게하고 transform함수가 반환하는 값으로 변경하도록 한다.\n",
    "x[ : , 1:3] = imputer.transform(X[ : , 1:3]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['France' 44.0 72000.0]\n",
      " ['Spain' 27.0 48000.0]\n",
      " ['Germany' 30.0 54000.0]\n",
      " ['Spain' 38.0 61000.0]\n",
      " ['Germany' 40.0 63777.77777777778]\n",
      " ['France' 35.0 58000.0]\n",
      " ['Spain' 38.77777777777778 52000.0]\n",
      " ['France' 48.0 79000.0]\n",
      " ['Germany' 50.0 83000.0]\n",
      " ['France' 37.0 67000.0]]\n"
     ]
    }
   ],
   "source": [
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CriG6VzVSjcK"
   },
   "source": [
    "## Encoding categorical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset에 categories (범주)가 있는 col이 하나 있음. (Country)\n",
    "# 머신러닝을 하면 이 문자열을 종속 변수 벡터에 반영하는 것을 어려워함\n",
    "# 그래서 이 문자열들로 이루어진 category를 숫자로 바꿔야 함 (encoding)\n",
    "# don't ) 프랑스 : 0, 독일 : 1, 미국 : 2...(encoding) -> 세 국가 사이에 숫자의 순서와 중요도가 있다고 인식할 수도 있음.\n",
    "# 이런 오해가 생기면 안 되니까 최대한 이런 상황을 피해야 함.\n",
    "\n",
    "# Do!!! ) One-Hot encoding (category형 data를 가진 새 dataset 전처리 할 때 유용함)\n",
    "# 'Country' col의 서로 다른 3개의 class에 (3개의 category) -> 맞게 3개의 col로 만들어줌\n",
    "# One-hot encoding은 각 나라에 이진 벡터를 생성하는 것으로 구성됨.\n",
    "    # ex) 프랑스 : 1 0 0 / 스페인 : 0 1 0 / 독일 : 0 0 1\n",
    "        # 이렇게되면 0,1만 존재하기 때문에 3국가 사이의 관계성이 없어짐. (better)\n",
    "        \n",
    "# label이 있는 col도 잊으면 안됨 (yes or no) -> 나중에 이것도 0,1로 바꿔 줘야함."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AhSpdQWeSsFh"
   },
   "source": [
    "### Encoding the Independent Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2개의 class를 one-hot encoding을 하는데 사용.\n",
    "# sklearn lib의 compose module 접근 권환을 가져옴, ColumnTransformer Class를 불러옴\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# 2개의 인자가 들어감\n",
    "    # 1. tranformer : 어떤 col의 index에 변환을 적용하고 싶은지 지정할 수 있는 인자\n",
    "        # 3개의 요소\n",
    "            # 1. 변환의 종류 : encoding해줄 것이기 때문에 'encoder' 입력\n",
    "            # 2. 이 encoding을 실행할 class의 이름 : OneHotEncoder\n",
    "            # 3. 대 괄호를 열고, oneHotEncoding을 적용할 col의 index를 넣어주면 됨. (변환하고 싶은 것)\n",
    "    # 2. remainder : 'salary'나 'age'처럼 변환이 적용되지 않고 유지되길 원하는 col을 지정해주는 인자\n",
    "        # code명 : passthrough -> 변환이 적용도지 않을 col을 지정해주는 코드명\n",
    "ct = ColumnTransformer(transformers = [( 'encoder', OneHotEncoder(), [0] )], remainder = 'passthrough' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature matrix x 연결 ( 아직 연결 되어있지는 않음 )\n",
    "\n",
    "# fit과 transform 한번에 가능, ColumnTransformer Class가 method를 가지고 있음.\n",
    "# x = ct.fit_transform(x) # return 한다는 것을 주의!\n",
    "\n",
    "# 또 하나 주의 할점\n",
    "# fit_transform은 np_array에 입력값을 반환하지 않음.하지만 우리가 만들 머신 모델에 np.array값은 필수임.\n",
    "    # 또한 이 기계 모델을 훈련시킬 때 'Fit'이라고 하는 훈련 기능을 사용할 것임. \n",
    "        # 이 훈련 기능이 'np.array'로 feature matrix x를 필요로 하니까 'fit_transform method' 출력값이\n",
    "            # np.array가 되야함. -> 따라서 np.array() 함수로 바꿔줘야 함\n",
    "x = np.array(ct.fit_transform(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0 1.0 0.0 0.0 44.0 72000.0]\n",
      " [1.0 0.0 0.0 1.0 27.0 48000.0]\n",
      " [1.0 0.0 1.0 0.0 30.0 54000.0]\n",
      " [1.0 0.0 0.0 1.0 38.0 61000.0]\n",
      " [1.0 0.0 1.0 0.0 40.0 63777.77777777778]\n",
      " [0.0 1.0 0.0 0.0 35.0 58000.0]\n",
      " [1.0 0.0 0.0 1.0 38.77777777777778 52000.0]\n",
      " [0.0 1.0 0.0 0.0 48.0 79000.0]\n",
      " [1.0 0.0 1.0 0.0 50.0 83000.0]\n",
      " [0.0 1.0 0.0 0.0 37.0 67000.0]]\n"
     ]
    }
   ],
   "source": [
    "print(x)\n",
    "# 3 국가가 oneHotEncoding을 통해 고유한 id를 갖게됨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DXh8oVSITIc6"
   },
   "source": [
    "### Encoding the Dependent Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# yes or no 2Classr가  경우는 Label Encoder 사용\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder() # 하나 뿐인 single vector이기에 어떤걸 encoding할지 명확하기 때문에 인자 필요없음.\n",
    "y =  le.fit_transform(y) # 이번에는 종속변벡터라 np.array가 필요없음, 나중에 머신 모델에 필요한 np.array가 될 필요가 없기때ㅛle.fit_transform(y) # 이번에는 종속변벡터라 np.array가 필요없음, 나중에 머신 모델에 필요한 np.array가 될 필요가 없기때le.fit_transform(y) # 이번에는 종속변벡터라 np.array가 필요없음, 나중에 머신 모델에 필요한 np.array가 될 필요가 없기때ㅛle.fit_transform(y) # 이번에는 종속변벡터라 np.array가 필요없음, 나중에 머신 모델에 필요한 np.array가 될 필요가 없기때ㅛ문"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 0 0 1 1 0 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qb_vcgm3qZKW"
   },
   "source": [
    "## Splitting the dataset into the Training set and Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data는 Training data 와 Test data로 나뉘어짐.\n",
    "    # Training data : 모델 학습에 사용되는 모든 데이터셋\n",
    "    # Test data : 오직 모델 평가만을 위해 사용되는 데이터셋\n",
    "\n",
    "# Question \n",
    "# feature Scaling은 dateset를 훈련 set과 테스트 set으로 분할하기 전과 후, 언제 적용해야하나요?\n",
    "\n",
    "# Answer\n",
    "# 훈련 set와 / 테스트 set로 분할한 후에 적용\n",
    "    #why? : 테스트 set은 머신러닝 모델을 평가할 완전히 새로운 세트가 되야 하기 때문\n",
    "        # train set에서 머신러닝 모델을 훈련시키고, 훈련이 끝난 후에 새로운 관측값을 배치하기 때문에 테스트 set는 훈련할 때 사용하는게 아님\n",
    "        # 실제 작업하는 데이터에는 테스트 세트가 있으면 안됨.\n",
    "            #분할 전 원본 데이터 set에 feature Scaling을 적용하는 것은 테스트 set에 '정보 유출'상황을 발생시킬 수 있음\n",
    "            #원래 새 관측값으로 새 데이터가 되어야 하지만, 이럴경우 테스트 set에서 가지고 와서는 안되는 정보를 가지고 오게 되는 것임.\n",
    "            \n",
    "    #두 개의 분리된 세트를 만듦\n",
    "        # Train set은 기존 관측값으로 기계모델을 훈련시킬 것임.\n",
    "        # Test set은 새로운 관측값으로 모델의 성능을 평가할 것임.\n",
    "            #여기서 새로운 관측값이란 미래에 갖게 될 데이터이자 미래 머신러닝 모델에 배치할 데이터를 말함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "#train_text_split 함수로 return될 4개의 변수를 생성함\n",
    "\n",
    "# 1. 훈련 set의 feature matrix인 'x_train'을 삽입\n",
    "    # OneHot encoding이 된 훈련 set의 'country'와 / 'age', 'salary'가 포함된 'x_train'\n",
    "# 2. 'x_test'는 테스트 세트의 feature matrix\n",
    "# 3. 'y_train'은 훈련 set의 종속 변수, 훈련 set에 있는 모든 고객들의 'Purchace'여부\n",
    "# 4. 'y_test'는 테스트 set에서 똑같이 고객들의 구매 여부를 포함.\n",
    "# 인자\n",
    "    # 3. split size : 데이터 세트를 훈련 세트와 테스트 세트로 분할할 때 크기를 다르게 나눠줄 것임.\n",
    "        # 훈련 세트에는 더 많은 관측값이 필요하고, 테스트 세트에는 덜 필요함.\n",
    "            # 훈련 세트에는 훨씬 많이 넣어서 미래 머신러닝 모델이 데이터 세트의 상관관계를 더 많이 학습하고 이해가능.\n",
    "                # 분할의 권장크기 : 훈련세트 - 80%, 테스트 세트 - 20% 관측값을 권장\n",
    "                    # 8명의 고객이 훈련 세트로 들어가고, 2명이 테스트 세트로 들어간다는 의미\n",
    "        # 분할하는 동안 random 요소들이 일어남. test값이 훈련 세트와 테스트 세트에 무작위로 분할되기 때문\n",
    "    # 4. 똑같이 random 요소가 추가된다는 사실을 확인하기 위해 random_state 추가\n",
    "        # 여기 seed를 고정해서 같은 분할을 가질 수 있게 하는 것임. \n",
    "            # 훈련 세트, 테스트 세트가 같을 수 있도록 Data Set 를 분할하는 코드\n",
    "                # 어떤 숫자값을 넣어도 그 기능은 똑같다.\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state=1 )\n",
    "    \n",
    "\n",
    "# 훈련 set의 feature : x_train\n",
    "# 훈련 set의 종속 변수인 'y_train'\n",
    "# 테스트 set의 feature matrix인 'x_test'\n",
    "# 테스트 set의 종속 변수인 'y_test'\n",
    "    #왜 이러한 자료들이 필요할까?\n",
    "    \n",
    "# 우리가 만들 미래의 머신러닝 모델이 형태를 입력값으로 요구함 (즉, 미래 머신러닝이 원하는 형태라는 것임)\n",
    "    #훈련할 때 'x-train', 'y_train'을 입력값으로 요구하고 'fit' method도 필요함\n",
    "        # 'inference'라고 하는 예측을 위해 이 모델들이 'x_test'를 예측할 것임.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.0 0.0 0.0 1.0 38.77777777777778 52000.0]\n",
      " [1.0 0.0 1.0 0.0 40.0 63777.77777777778]\n",
      " [0.0 1.0 0.0 0.0 44.0 72000.0]\n",
      " [1.0 0.0 0.0 1.0 38.0 61000.0]\n",
      " [1.0 0.0 0.0 1.0 27.0 48000.0]\n",
      " [0.0 1.0 0.0 0.0 48.0 79000.0]\n",
      " [1.0 0.0 1.0 0.0 50.0 83000.0]\n",
      " [0.0 1.0 0.0 0.0 35.0 58000.0]]\n"
     ]
    }
   ],
   "source": [
    "print(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.0 0.0 1.0 0.0 30.0 54000.0]\n",
      " [0.0 1.0 0.0 0.0 37.0 67000.0]]\n"
     ]
    }
   ],
   "source": [
    "print(x_test)\n",
    "# 첫 3개의 col에 가변수가 있음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['No' 'Yes' 'No' 'No' 'Yes' 'Yes' 'No' 'Yes']\n"
     ]
    }
   ],
   "source": [
    "print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['No' 'Yes']\n"
     ]
    }
   ],
   "source": [
    "print(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TpGqbS4TqkIR"
   },
   "source": [
    "## Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#모든 변수나 feature을 scaling하는 도구, 모든 값이 같은 단위가 되게끔 함.\n",
    "# feature이 다른 하나를 지배해서 머신러닝 모델에 의해 무시되는 일을 막기위한 것\n",
    "# Scaling하기위해 feature의 평균과 표준편차를 구하는 기술.\n",
    "\n",
    "# 궁극적으로 ML성능을 좋게하기 위해 시도하는 기법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 다시한번! feature Scaling은 왜 사용하는가?\n",
    "# 모든 feature을 동일한 크기로 조쟁해주는 도구 ( featrue : Xi )\n",
    " # 일부 machine model 에서 일부 feature이 다른 feature에 의해 지배되어 \n",
    "  # machine model에 인식되지 않는 상황을 방지하기 위해 사용.\n",
    "    # 모든 모델에 사용되는 것은 아님. (일부만)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 주로 사용되는 2개의 feature Scaling 기법 (모든 feature을 동일한 크기(Scale)로 조정)\n",
    " # 1. Standardisation (표준화) : 모든 feature의 값을 -3에서 +3 사이에 놓을 것임.\n",
    " # 2. Normalisation (정규화 ) : feature의 모든 값은 0과 1사이가 된다. (분모가 항상 크기 때문)\n",
    "    # Question : 표준화와 정규화 중 무엇을 선택해야하나요?\n",
    "        # 정규화는 대부분 feature에 정규 분포를 따르는 특정 상호아에 권장됨. 이 경우 feature scaling이 유용.\n",
    "        # 표준화는 항상 잘 작동, 언제나 제 역할을 함. (언제나 작동하는 기법)\n",
    "            # 표준화를 추천 : 항상 적절한 feature scaling을 수행하여 훈련 process를 개선가능."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train과 x_test에 feature Scaling은 따로 적용할 것이다.\n",
    " # Scaler는 x_train에만 적용될 것이다.\n",
    "  # 그리고 x_test로 transform해서 feature Scaling을 적용할 건데\n",
    "   # x_test는 training에 있어서는 안되고, 오직 실제 환경에 들어갈 때만 필요.\n",
    "    # 따라서 feature Scaling 도구를 Test set에 맞추는 건 불가능.\n",
    "     # Test set에 들어가는 feature을 맞추는 건 전체 set의 feature에서 평균과 표준편차를 가져오겠다는 의미인데\n",
    "        # x_test는 새로운 거라 이렇게 할 권리가 없기 때문\n",
    "        \n",
    "# 결론\n",
    " # x_train값의 평균과 표준편차를 가지고와서\n",
    "    # x_train의 모든 값을 변환하기 위해 표준화 or 정규화 식을 사용하고\n",
    "     # 같은 x_train값의 평균과 표준편차를 같은 공식에 적용해서 x_test값을 Scaling해주는 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature scaling 적용\n",
    "# 모든 도구가 들어있는 standard scaler Class 사용. 훈련세트와 테스트세트의 feature matrix모두에 표준화 수행해주는 도구\n",
    "# preprocessing : 전처리 모듈, standard scaler 클래스가 여기에 있음.\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "sc = StandardScaler() # 인자는 안 넣어도됨. 단순히 평균과 표준편차를 구해서 Standardisation하기 때문. feature의 모든 값에 공식 적용\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question\n",
    "# feature matirx의 가변수(dumy variables)에 feature scaling / Standardisation을 적용해야 하는가?\n",
    "\n",
    "# Answer : no (안 하는게 좋음, 숫자값에만 적용해주자)\n",
    " # Why? : 표준화의 목적은 모둔 feature을 변환시켜 모든 값들을 같은 범위에 놓기 위한 것. \n",
    "    # 가변수는 이미 0,1 로 이루어져있기 때문에 이미 -3 ~ +3 의 값을 가지고 있음. 표준화로 더 해줄게 없음.\n",
    "        # 오히려 상황을 나쁘게 만듦. 여전히 이 값들을 -3 ~ +3 사이로 변환하려고 할 것이기 때문\n",
    "             # 어떤 나라가 관측치에 상응하는지 정보를 잃게 됨\n",
    "                # 그렇기에 모델의 해석 능력을 유지할 수 있도록 가변수는 그냥 둘 것임."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit method를 적용할 것임\n",
    "    # fit method : x_train의 각 feature 모두의 평균, 표준편차를 계산\n",
    "    # transform : standardisation 공식을 적용해서 여기 있는 각 feature값을 결과로 나온 이 값으로 변환\n",
    "        # fit과 transform의 차이를 이해하는 것은 중요.\n",
    "        \n",
    "# fit_transform 함수로 한번에 실행가능. (StanrdScaler Class)\n",
    "x_train[ : , 3:] = sc.fit_transform(x_train[:, 3:])\n",
    "\n",
    "# 그 다음으로 test set에 있는 feature matrix를 transform해야 함. (x_test를 의미함)\n",
    "    # 하지만 test set은 나중에 프로덕션에서 얻게 될 new Data이기 때문에\n",
    "        # transform method만 적용할 것임.\n",
    "            # ML 모델이 훈련된 후 train set에 사용된 것과 똑같은 Scaler를 사용해 Scaling해야하기 때문\n",
    "                # 지금 x_test에 'fit_transform' method를 적용하면 새로운 Scaler를 가져옴\n",
    "                 #이건 말이안됨. ML 모델이 훈련된 후, x_test는 예측에 반환되는 'predict' 기능의 입력값이 됨.\n",
    "                    # ML 모델이 train set에 적용된 Scaler를 사용해 훈련될거라\n",
    "                     # 모델이 훈련된 방식과 맞아떨어지는 'predict'값을 만들려면\n",
    "                        # train set에 사용된 것과 같은 scaler를 test set에 적용해야 함.\n",
    "                         #그래서 같은 transform과 x_test에 적용된 method와 관련된 predict를 갖는다.\n",
    "                            \n",
    "x_test[ :, 3:] = sc.transform(x_test[ : , 3:]) \n",
    "#transform method를 train set에 적용된 같은 Scaler (sc) 에서 가져옴. (훈련의 일부임)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.0 0.0 1.0 -0.7745966692414834 -1.4661817944830124 -0.9069571034860727]\n",
      " [0.0 1.0 0.0 -0.7745966692414834 -0.44973664397484414 0.2056403393225306]]\n"
     ]
    }
   ],
   "source": [
    "print(x_test)\n",
    "\n",
    "# age, salary 변수가 변함. 모든 변수가 같은 크기가 됐음.\n",
    "# 이게 특정 ML 모델 훈련을 향상시키거나 최적화 해줄 것임."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.0 0.0 0.0 1.2909944487358056 -0.19159184384578545 -1.0781259408412425]\n",
      " [1.0 0.0 1.0 -0.7745966692414834 -0.014117293757057777\n",
      "  -0.07013167641635372]\n",
      " [0.0 1.0 0.0 -0.7745966692414834 0.566708506533324 0.633562432710455]\n",
      " [1.0 0.0 0.0 1.2909944487358056 -0.30453019390224867\n",
      "  -0.30786617274297867]\n",
      " [1.0 0.0 0.0 1.2909944487358056 -1.9018011447007988 -1.420463615551582]\n",
      " [0.0 1.0 0.0 -0.7745966692414834 1.1475343068237058 1.232653363453549]\n",
      " [1.0 0.0 1.0 -0.7745966692414834 1.4379472069688968 1.5749910381638885]\n",
      " [0.0 1.0 0.0 -0.7745966692414834 -0.7401495441200351 -0.5646194287757332]]\n"
     ]
    }
   ],
   "source": [
    "print(x_train)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "data_preprocessing_tools.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
